{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Activation, Cropping2D, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Concatenate, Add\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.utils import layer_utils, plot_model, to_categorical, multi_gpu_model, normalize\n",
    "import keras.backend as K\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import keras\n",
    "#import models\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "import math\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, Callback, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "\n",
    "#Import dataset\n",
    "\n",
    "importdataset=np.load(\"Numpy arrays/images.npy\")\n",
    "importlabels=to_categorical(np.load(\"Numpy arrays/masks.npy\"))\n",
    "\n",
    "print(importdataset.shape)\n",
    "print(importdataset.dtype)\n",
    "print(importlabels.shape)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split each image into multiple smaller images\n",
    "\n",
    "xsplit=1\n",
    "ysplit=1\n",
    "\n",
    "dataset=np.zeros((importdataset.shape[0]*xsplit*ysplit,int(importdataset.shape[1]/ysplit),int(importdataset.shape[2]/xsplit),importdataset.shape[3]), dtype = 'float32')\n",
    "labels=np.zeros((importlabels.shape[0]*xsplit*ysplit,int(importlabels.shape[1]/ysplit),int(importlabels.shape[2]/xsplit),importlabels.shape[3]), dtype = 'bool')\n",
    "\n",
    "\n",
    "for i in range(xsplit*ysplit):\n",
    "    print(i+1,'out of',xsplit*ysplit)\n",
    "    dataset[i*importdataset.shape[0]:(i+1)*importdataset.shape[0],:,:,:]=importdataset[:,i%ysplit*int(importdataset.shape[1]/ysplit):(i%ysplit+1)*int(importdataset.shape[1]/ysplit),math.floor(i/ysplit)%xsplit*int(importdataset.shape[2]/xsplit):(math.floor(i/ysplit)%xsplit+1)*int(importdataset.shape[2]/xsplit),:].astype('float32')/255\n",
    "    labels[i*importlabels.shape[0]:(i+1)*importlabels.shape[0],:,:,:]=importlabels[:,i%ysplit*int(importlabels.shape[1]/ysplit):(i%ysplit+1)*int(importlabels.shape[1]/ysplit),math.floor(i/ysplit)%xsplit*int(importlabels.shape[2]/xsplit):(math.floor(i/ysplit)%xsplit+1)*int(importlabels.shape[2]/xsplit),:].astype('bool')\n",
    "\n",
    "NoOfImages=dataset.shape[0]\n",
    "\n",
    "#x_train=dataset[:int(NoOfImages*2/3),:,:,:]\n",
    "#x_test=dataset[int(NoOfImages*2/3):NoOfImages,:,:,:]\n",
    "#y_train=to_categorical(labels[:int(NoOfImages*2/3),:,:])\n",
    "#y_test=to_categorical(labels[int(NoOfImages*2/3):NoOfImages,:,:])\n",
    "#importdataset\n",
    "#importlabels\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del importdataset\n",
    "\n",
    "#Resize images to 224x224\n",
    "\n",
    "resized_dataset=np.zeros((dataset.shape[0],224,224,3), dtype='float32')\n",
    "resized_labels=np.zeros((labels.shape[0],224,224,2), dtype='float32')\n",
    "for i in range(dataset.shape[0]):\n",
    "    resized_dataset[i] = cv2.resize(dataset[i,...], dsize=(224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "    resized_labels[i] = cv2.resize(labels[i,...].astype('float32'), dsize=(224, 224), interpolation=cv2.INTER_CUBIC).astype('bool')\n",
    "\n",
    "del dataset\n",
    "print(np.amax(resized_dataset))\n",
    "#dataset=2*dataset[labels[..., 1].any(axis=((1,2)))].astype('float32')-1\n",
    "dataset=dataset[labels[..., 1].any(axis=((1,2)))].astype('float32')\n",
    "labels=labels[labels[..., 1].any(axis=((1,2)))]\n",
    "resized_dataset=2*resized_dataset-1\n",
    "#dataset=dataset[labels[..., 1].any(axis=((1,2)))].astype('float32')\n",
    "print(np.amax(resized_dataset))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "print(resized_dataset.dtype)\n",
    "print(resized_labels.dtype)\n",
    "print(resized_dataset.shape)\n",
    "print(resized_labels.shape)\n",
    "a=np.sum(resized_labels[...,0])\n",
    "b=np.sum(resized_labels[...,1])\n",
    "print(a)\n",
    "print(b)\n",
    "print(a/(b+a))\n",
    "print(resized_dataset[68,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "\n",
    "f = plt.figure()\n",
    "image=109\n",
    "f.add_subplot(1,2, 1)\n",
    "#plt.imshow(resized_dataset[image,:,:,:].astype('float32')/2+[.5])\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(resized_labels[image,:,:,1])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resunet with variable parameter number\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_floatx('float32')\n",
    "smallest_layer=64\n",
    "\n",
    "def RESUNET(input_shape, n_classes):\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    ###encoding block 1\n",
    "    iden1 = Conv2D(smallest_layer, 1, activation = None, padding='same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(inputs) #200\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1) #200\n",
    "    add1 = Add()([iden1,conv1])\n",
    "    pool1 = MaxPooling2D()(add1) #200 -> 100\n",
    "    print(add1)\n",
    "\n",
    "    ###encoding block2\n",
    "    iden2 = Conv2D(smallest_layer*2, 1, activation = None, padding='same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = BatchNormalization()(pool1)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    add2 = Add()([iden2,conv2])\n",
    "    pool2 = MaxPooling2D()(add2) #100 ->50\n",
    "    print (add2)\n",
    "\n",
    "    ###encoding block3\n",
    "    iden3 = Conv2D(smallest_layer*4, 1, activation = None, padding='same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv3 = BatchNormalization()(pool2)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    add3 = Add()([iden3,conv3])\n",
    "    pool3= MaxPooling2D()(add3) #50->25\n",
    "    print (add3)\n",
    "\n",
    "    ###encoding block4\n",
    "    iden4 = Conv2D(smallest_layer*8, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(pool3)\n",
    "    conv4 = BatchNormalization()(pool3)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv4) #200\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv4) #200\n",
    "    add4 = Add()([iden4,conv4])\n",
    "    drop4 = Dropout(0.5)(add4)\n",
    "    pool4 = MaxPooling2D()(drop4) #25->12\n",
    "    print (pool4)\n",
    "\n",
    "    ###bridge\n",
    "    conv5 = BatchNormalization()(pool4)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*16, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*16, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    print (conv5)\n",
    "\n",
    "    ###decoding block1\n",
    "    up6 = UpSampling2D()(drop5) #12->24\n",
    "    up6 = ZeroPadding2D(((1,0),(1,0)))(up6) #24->25\n",
    "    concat6 = Concatenate(axis=3)([up6,add4])\n",
    "    iden6 = Conv2D(smallest_layer*8, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(concat6)\n",
    "    conv6 = BatchNormalization()(concat6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv6) #200\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv6) #200\n",
    "    add6 = Add()([iden6,conv6])\n",
    "\n",
    "    ###decoding block2\n",
    "    up7 = UpSampling2D()(add6) #25->50\n",
    "    up7 = ZeroPadding2D(((0,0),(0,0)))(up7) #24->25\n",
    "    concat7 = Concatenate(axis=3)([up7,add3])\n",
    "    iden7 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(concat7)\n",
    "    conv7 = BatchNormalization()(concat7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv7) #200\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv7) #200\n",
    "    add7 = Add()([iden7,conv7])\n",
    "\n",
    "    ###decoding block3\n",
    "    up8 = UpSampling2D()(add7) #50->100\n",
    "    up8 = ZeroPadding2D(((0,0),(0,0)))(up8) #24->25\n",
    "    concat8 = Concatenate(axis=3)([up8,add2])\n",
    "    iden8 = Conv2D(smallest_layer*2, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(concat8)\n",
    "    conv8 = BatchNormalization()(concat8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv8) #200\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv8) #200\n",
    "    add8 = Add()([iden8,conv8])\n",
    "\n",
    "    ###decoding block4\n",
    "    up9 = UpSampling2D()(add8) #100->200\n",
    "    up9 = ZeroPadding2D(((0,0),(0,0)))(up9) #24->25\n",
    "    concat9 = Concatenate(axis=3)([up9,add1])\n",
    "    iden9 = Conv2D(smallest_layer,1,activation=None, padding='same', kernel_initializer = 'he_normal')(concat9)\n",
    "    conv9 = BatchNormalization()(concat9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv9) #200\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv9) #200\n",
    "    add9 = Add()([iden9,conv9])\n",
    "\n",
    "    conv10 = Conv2D(n_classes, 3, activation ='sigmoid', padding = 'same', kernel_initializer = 'he_normal')(add9)\n",
    "    #sigmoid probably too strong an activation\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resunet with less layers\n",
    "\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_floatx('float32')\n",
    "smallest_layer=32\n",
    "\n",
    "def RESUNETMEDIUM(input_shape, n_classes):\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    ###encoding block 1\n",
    "    iden1 = Conv2D(smallest_layer, 1, activation = None, padding='same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(inputs) #200\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1) #200\n",
    "    add1 = Add()([iden1,conv1])\n",
    "    pool1 = MaxPooling2D((3,3))(add1) #224 -> 74\n",
    "    print(add1)\n",
    "\n",
    "    ###encoding block2\n",
    "    iden2 = Conv2D(smallest_layer*2, 1, activation = None, padding='same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv2 = BatchNormalization()(pool1)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    add2 = Add()([iden2,conv2])\n",
    "    pool2 = MaxPooling2D((3,3))(add2) #74 -> 24\n",
    "    print (add2)\n",
    "\n",
    "    ###encoding block4\n",
    "    iden4 = Conv2D(smallest_layer*8, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(pool2)\n",
    "    conv4 = BatchNormalization()(pool2)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv4) #200\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv4) #200\n",
    "    add4 = Add()([iden4,conv4])\n",
    "    drop4 = Dropout(0.5)(add4)\n",
    "    pool4 = MaxPooling2D((3,3))(drop4) #24 -> 8\n",
    "    print (pool4)\n",
    "\n",
    "    ###bridge\n",
    "    conv5 = BatchNormalization()(pool4)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*16, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*16, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    print (conv5)\n",
    "\n",
    "    ###decoding block1\n",
    "    up6 = UpSampling2D((3,3))(drop5) #8->24\n",
    "    #up6 = ZeroPadding2D(((1,0),(1,0)))(up6) #24->25\n",
    "    concat6 = Concatenate(axis=3)([up6,add4])\n",
    "    iden6 = Conv2D(smallest_layer*8, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(concat6)\n",
    "    conv6 = BatchNormalization()(concat6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv6) #200\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    conv6 = Conv2D(smallest_layer*8, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv6) #200\n",
    "    add6 = Add()([iden6,conv6])\n",
    "\n",
    "    ###decoding block3\n",
    "    up8 = UpSampling2D((3,3))(add6) #24 -> 72\n",
    "    up8 = ZeroPadding2D(((0,2),(0,2)))(up8) #72->74\n",
    "    concat8 = Concatenate(axis=3)([up8,add2])\n",
    "    iden8 = Conv2D(smallest_layer*2, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(concat8)\n",
    "    conv8 = BatchNormalization()(concat8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv8) #200\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    conv8 = Conv2D(smallest_layer*2, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv8) #200\n",
    "    add8 = Add()([iden8,conv8])\n",
    "\n",
    "    ###decoding block4\n",
    "    up9 = UpSampling2D((3,3))(add8) #74 -> 222\n",
    "    up9 = ZeroPadding2D(((0,2),(0,2)))(up9) #222->224\n",
    "    concat9 = Concatenate(axis=3)([up9,add1])\n",
    "    iden9 = Conv2D(smallest_layer,1,activation=None, padding='same', kernel_initializer = 'he_normal')(concat9)\n",
    "    conv9 = BatchNormalization()(concat9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv9) #200\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv9) #200\n",
    "    add9 = Add()([iden9,conv9])\n",
    "\n",
    "    conv10 = Conv2D(n_classes, 3, activation ='sigmoid', padding = 'same', kernel_initializer = 'he_normal')(add9)\n",
    "    #sigmoid probably too strong an activation\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    return model\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resunet with even less layers\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_floatx('float32')\n",
    "smallest_layer=42\n",
    "\n",
    "def RESUNETSMALL(input_shape, n_classes):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    ###encoding block 1\n",
    "    iden1 = Conv2D(smallest_layer, 1, activation = None, padding='same', kernel_initializer = 'he_normal')(inputs)\n",
    "    conv1 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(inputs) #200\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1) #200\n",
    "    add1 = Add()([iden1,conv1])\n",
    "    pool1 = MaxPooling2D((4,4))(add1) #224 -> 56\n",
    "    print(add1)\n",
    "\n",
    "    ###encoding block3\n",
    "    iden3 = Conv2D(smallest_layer*4, 1, activation = None, padding='same', kernel_initializer = 'he_normal')(pool1)\n",
    "    conv3 = BatchNormalization()(pool1)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    add3 = Add()([iden3,conv3])\n",
    "    #drop3 = Dropout(0.5)(add3)\n",
    "    pool3= MaxPooling2D((4,4))(add3)#(drop3) #56 -> 14\n",
    "    print (add3)\n",
    "\n",
    "    ###bridge\n",
    "    conv5 = BatchNormalization()(pool3)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*16, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*16, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    print (conv5)\n",
    "\n",
    "    ###decoding block2\n",
    "    up7 = UpSampling2D((4,4))(drop5) #40-> 160\n",
    "    up7 = ZeroPadding2D(((0,0),(0,0)))(up7) #160->162\n",
    "    concat7 = Concatenate(axis=3)([up7,add3])\n",
    "    iden7 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(concat7)\n",
    "    conv7 = BatchNormalization()(concat7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv7) #200\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    conv7 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv7) #200\n",
    "    add7 = Add()([iden7,conv7])\n",
    "\n",
    "    ###decoding block4\n",
    "    up9 = UpSampling2D((4,4))(add7) #162->648\n",
    "    up9 = ZeroPadding2D(((0,0),(0,0)))(up9) #24->25\n",
    "    concat9 = Concatenate(axis=3)([up9,add1])\n",
    "    iden9 = Conv2D(smallest_layer,1,activation=None, padding='same', kernel_initializer = 'he_normal')(concat9)\n",
    "    conv9 = BatchNormalization()(concat9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv9) #200\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    conv9 = Conv2D(smallest_layer, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv9) #200\n",
    "    add9 = Add()([iden9,conv9])\n",
    "\n",
    "    conv10 = Conv2D(n_classes, 3, activation ='sigmoid', padding = 'same', kernel_initializer = 'he_normal')(add9)\n",
    "    #sigmoid probably too strong an activation\n",
    "\n",
    "    model = Model(input = inputs, output = conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified Resnet152\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Flatten, Activation, add\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(3000)\n",
    "\n",
    "trainable = 'False'\n",
    "\n",
    "class Scale(Layer):\n",
    "    '''Custom Layer for ResNet used for BatchNormalization.\n",
    "    \n",
    "    Learns a set of weights and biases used for scaling the input data.\n",
    "    the output consists simply in an element-wise multiplication of the input\n",
    "    and a sum of a set of constants:\n",
    "        out = in * gamma + beta,\n",
    "    where 'gamma' and 'beta' are the weights and biases larned.\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "    '''\n",
    "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "        self.gamma_init = initializers.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        self.gamma = K.variable(self.gamma_init(shape), name='%s_gamma'%self.name)\n",
    "        self.beta = K.variable(self.beta_init(shape), name='%s_beta'%self.name)\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    '''The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=False, trainable=trainable)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter2, (kernel_size, kernel_size), name=conv_name_base + '2b', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    x = add([x, input_tensor], name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    '''conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', use_bias=False, trainable=trainable)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter2, (kernel_size, kernel_size),\n",
    "                      name=conv_name_base + '2b', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(nb_filter3, (1, 1), strides=strides,\n",
    "                             name=conv_name_base + '1', use_bias=False, trainable=trainable)(input_tensor)\n",
    "    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "    shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)\n",
    "\n",
    "    x = add([x, shortcut], name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet152_model(n_classes, weights_path=None):\n",
    "    '''Instantiate the ResNet152 architecture,\n",
    "    # Arguments\n",
    "        weights_path: path to pretrained weight file\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        bn_axis = 3\n",
    "        img_input = Input(shape=(224, 224, 3), name='data')\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "        img_input = Input(shape=(3, 224, 224), name='data')\n",
    "            \n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Scale(axis=bn_axis, name='scale_conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    for i in range(1,8):\n",
    "        x = identity_block(x, 3, [128, 128, 512], stage=3, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    for i in range(1,36):\n",
    "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    #for classification\n",
    "    #x_fc = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "    #x_fc = Flatten()(x_fc)\n",
    "    #x_fc = Dense(1000, activation='softmax', name='fc1000')(x_fc)\n",
    "\n",
    "    #new section for segmentation\n",
    "    up1 = UpSampling2D((2,2))(x) #7 -> 14\n",
    "    #up1 = ZeroPadding2D(((0,2),(0,2)))(up1) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden1 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up1)#concat1\n",
    "    conv1 = BatchNormalization()(up1)#concat1\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1) #200\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1) #200\n",
    "    add1 = Add()([iden1,conv1])\n",
    "    \n",
    "    up2 = UpSampling2D((2,2))(add1) #14 -> 28\n",
    "    #up2 = ZeroPadding2D(((0,2),(0,2)))(up2) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden2 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up2)#concat2\n",
    "    conv2 = BatchNormalization()(up2)#concat2\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    add2 = Add()([iden2,conv2])\n",
    "    \n",
    "    up3 = UpSampling2D((2,2))(add2) #28 -> 56\n",
    "    #up2 = ZeroPadding2D(((0,2),(0,2)))(up2) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden3 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up3)#concat3\n",
    "    conv3 = BatchNormalization()(up3)#concat3\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    add3 = Add()([iden3,conv3])\n",
    "    \n",
    "    up4 = UpSampling2D((2,2))(add3) #56 -> 128\n",
    "    #up2 = ZeroPadding2D(((0,2),(0,2)))(up2) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden4 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up4)#concat3\n",
    "    conv4 = BatchNormalization()(up4)#concat3\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv4) #200\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    conv4 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv4) #200\n",
    "    add4 = Add()([iden4,conv4])\n",
    "    \n",
    "    up5 = UpSampling2D((2,2))(add4) #128 -> 224\n",
    "    #up2 = ZeroPadding2D(((0,2),(0,2)))(up2) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden5 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up5)#concat3\n",
    "    conv5 = BatchNormalization()(up5)#concat3\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    conv5 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv5) #200\n",
    "    add5 = Add()([iden5,conv5])\n",
    "    \n",
    "    conv6 = Conv2D(n_classes, 3, activation ='sigmoid', padding = 'same', kernel_initializer = 'he_normal')(add5)\n",
    "    \n",
    "    model = Model(img_input, conv6)\n",
    "    \n",
    "     # load weights\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modified Resnet152 again (?)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Flatten, Activation, add\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import backend as K\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(3000)\n",
    "\n",
    "trainable = 'False'\n",
    "\n",
    "class Scale(Layer):\n",
    "    '''Custom Layer for ResNet used for BatchNormalization.\n",
    "    \n",
    "    Learns a set of weights and biases used for scaling the input data.\n",
    "    the output consists simply in an element-wise multiplication of the input\n",
    "    and a sum of a set of constants:\n",
    "        out = in * gamma + beta,\n",
    "    where 'gamma' and 'beta' are the weights and biases larned.\n",
    "    # Arguments\n",
    "        axis: integer, axis along which to normalize in mode 0. For instance,\n",
    "            if your input tensor has shape (samples, channels, rows, cols),\n",
    "            set axis to 1 to normalize per feature map (channels axis).\n",
    "        momentum: momentum in the computation of the\n",
    "            exponential average of the mean and standard deviation\n",
    "            of the data, for feature-wise normalization.\n",
    "        weights: Initialization weights.\n",
    "            List of 2 Numpy arrays, with shapes:\n",
    "            `[(input_shape,), (input_shape,)]`\n",
    "        beta_init: name of initialization function for shift parameter\n",
    "            (see [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "        gamma_init: name of initialization function for scale parameter (see\n",
    "            [initializers](../initializers.md)), or alternatively,\n",
    "            Theano/TensorFlow function to use for weights initialization.\n",
    "            This parameter is only relevant if you don't pass a `weights` argument.\n",
    "    '''\n",
    "    def __init__(self, weights=None, axis=-1, momentum = 0.9, beta_init='zero', gamma_init='one', **kwargs):\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "        self.beta_init = initializers.get(beta_init)\n",
    "        self.gamma_init = initializers.get(gamma_init)\n",
    "        self.initial_weights = weights\n",
    "        super(Scale, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (int(input_shape[self.axis]),)\n",
    "\n",
    "        self.gamma = K.variable(self.gamma_init(shape), name='%s_gamma'%self.name)\n",
    "        self.beta = K.variable(self.beta_init(shape), name='%s_beta'%self.name)\n",
    "        self.trainable_weights = [self.gamma, self.beta]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = self.input_spec[0].shape\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis]\n",
    "\n",
    "        out = K.reshape(self.gamma, broadcast_shape) * x + K.reshape(self.beta, broadcast_shape)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"momentum\": self.momentum, \"axis\": self.axis}\n",
    "        base_config = super(Scale, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def identity_block(input_tensor, kernel_size, filters, stage, block):\n",
    "    '''The identity_block is the block that has no conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(nb_filter1, (1, 1), name=conv_name_base + '2a', use_bias=False, trainable=trainable)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter2, (kernel_size, kernel_size), name=conv_name_base + '2b', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    x = add([x, input_tensor], name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2)):\n",
    "    '''conv_block is the block that has a conv layer at shortcut\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: defualt 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the nb_filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    Note that from stage 3, the first conv layer at main path is with subsample=(2,2)\n",
    "    And the shortcut should have subsample=(2,2) as well\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "    nb_filter1, nb_filter2, nb_filter3 = filters\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "    scale_name_base = 'scale' + str(stage) + block + '_branch'\n",
    "\n",
    "    x = Conv2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', use_bias=False, trainable=trainable)(input_tensor)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2a')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2a')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2a_relu')(x)\n",
    "\n",
    "    x = ZeroPadding2D((1, 1), name=conv_name_base + '2b_zeropadding')(x)\n",
    "    x = Conv2D(nb_filter2, (kernel_size, kernel_size),\n",
    "                      name=conv_name_base + '2b', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2b')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2b')(x)\n",
    "    x = Activation('relu', name=conv_name_base + '2b_relu')(x)\n",
    "\n",
    "    x = Conv2D(nb_filter3, (1, 1), name=conv_name_base + '2c', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '2c')(x)\n",
    "    x = Scale(axis=bn_axis, name=scale_name_base + '2c')(x)\n",
    "\n",
    "    shortcut = Conv2D(nb_filter3, (1, 1), strides=strides,\n",
    "                             name=conv_name_base + '1', use_bias=False, trainable=trainable)(input_tensor)\n",
    "    shortcut = BatchNormalization(epsilon=eps, axis=bn_axis, name=bn_name_base + '1')(shortcut)\n",
    "    shortcut = Scale(axis=bn_axis, name=scale_name_base + '1')(shortcut)\n",
    "\n",
    "    x = add([x, shortcut], name='res' + str(stage) + block)\n",
    "    x = Activation('relu', name='res' + str(stage) + block + '_relu')(x)\n",
    "    return x\n",
    "\n",
    "def resnet152_model(n_classes, weights_path=None):\n",
    "    '''Instantiate the ResNet152 architecture,\n",
    "    # Arguments\n",
    "        weights_path: path to pretrained weight file\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    '''\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    # Handle Dimension Ordering for different backends\n",
    "    global bn_axis\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        bn_axis = 3\n",
    "        img_input = Input(shape=(224, 224, 3), name='data')\n",
    "    else:\n",
    "        bn_axis = 1\n",
    "        img_input = Input(shape=(3, 224, 224), name='data')\n",
    "            \n",
    "    x = ZeroPadding2D((3, 3), name='conv1_zeropadding')(img_input)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', use_bias=False, trainable=trainable)(x)\n",
    "    x = BatchNormalization(epsilon=eps, axis=bn_axis, name='bn_conv1')(x)\n",
    "    x = Scale(axis=bn_axis, name='scale_conv1')(x)\n",
    "    x = Activation('relu', name='conv1_relu')(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), name='pool1')(x)\n",
    "\n",
    "    x = conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1))\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='b')\n",
    "    x = identity_block(x, 3, [64, 64, 256], stage=2, block='c')\n",
    "\n",
    "    x = conv_block(x, 3, [128, 128, 512], stage=3, block='a')\n",
    "    for i in range(1,8):\n",
    "        x = identity_block(x, 3, [128, 128, 512], stage=3, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [256, 256, 1024], stage=4, block='a')\n",
    "    for i in range(1,36):\n",
    "        x = identity_block(x, 3, [256, 256, 1024], stage=4, block='b'+str(i))\n",
    "\n",
    "    x = conv_block(x, 3, [512, 512, 2048], stage=5, block='a')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='b')\n",
    "    x = identity_block(x, 3, [512, 512, 2048], stage=5, block='c')\n",
    "\n",
    "    #for classification\n",
    "    #x_fc = AveragePooling2D((7, 7), name='avg_pool')(x)\n",
    "    #x_fc = Flatten()(x_fc)\n",
    "    #x_fc = Dense(1000, activation='softmax', name='fc1000')(x_fc)\n",
    "\n",
    "    #new section for segmentation\n",
    "    up1 = UpSampling2D((2,2))(x) #7 -> 14\n",
    "    #up1 = ZeroPadding2D(((0,2),(0,2)))(up1) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden1 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up1)#concat1\n",
    "    conv1 = BatchNormalization()(up1)#concat1\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1) #200\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    conv1 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv1) #200\n",
    "    add1 = Add()([iden1,conv1])\n",
    "    \n",
    "    up2 = UpSampling2D((4,4))(add1) #14 -> 56\n",
    "    #up2 = ZeroPadding2D(((0,2),(0,2)))(up2) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden2 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up2)#concat2\n",
    "    conv2 = BatchNormalization()(up2)#concat2\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    conv2 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv2) #200\n",
    "    add2 = Add()([iden2,conv2])\n",
    "    \n",
    "    up3 = UpSampling2D((4,4))(add2) #56 -> 224\n",
    "    #up2 = ZeroPadding2D(((0,2),(0,2)))(up2) #160->162\n",
    "    #concat1 = Concatenate(axis=3)([up7,add3])\n",
    "    iden3 = Conv2D(smallest_layer*4, 1, activation=None, padding='same', kernel_initializer = 'he_normal')(up3)#concat3\n",
    "    conv3 = BatchNormalization()(up3)#concat3\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "    conv3 = Conv2D(smallest_layer*4, 3, activation = None, padding = 'same', kernel_initializer = 'he_normal')(conv3) #200\n",
    "    add3 = Add()([iden3,conv3])\n",
    "    \n",
    "    conv4 = Conv2D(n_classes, 3, activation ='sigmoid', padding = 'same', kernel_initializer = 'he_normal')(add3)\n",
    "    \n",
    "    model = Model(img_input, conv4)\n",
    "    \n",
    "     # load weights\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)#, by_name=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dice coefficient\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A weighted version of categorical_crossentropy for keras (2.0.6). This lets you apply a weight to unbalanced classes.\n",
    "@url: https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n",
    "@author: wassname\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying lots of loss functions\n",
    "def create_weighted_binary_crossentropy(zero_weight, one_weight):\n",
    "\n",
    "    def weighted_binary_crossentropy(y_true, y_pred):\n",
    "\n",
    "        # Original binary crossentropy (see losses.py):\n",
    "        # K.mean(K.binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "        # Calculate the binary crossentropy\n",
    "        b_ce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "        # Apply the weights\n",
    "        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight\n",
    "        weighted_b_ce = weight_vector * b_ce\n",
    "\n",
    "        # Return the mean error\n",
    "        return K.mean(weighted_b_ce)\n",
    "\n",
    "    return weighted_binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    \"\"\"Jaccard distance for semantic segmentation.\n",
    "    Also known as the intersection-over-union loss.\n",
    "    This loss is useful when you have unbalanced numbers of pixels within an image\n",
    "    because it gives all classes equal weight. However, it is not the defacto\n",
    "    standard for image segmentation.\n",
    "    For example, assume you are trying to predict if\n",
    "    each pixel is cat, dog, or background.\n",
    "    You have 80% background pixels, 10% dog, and 10% cat.\n",
    "    If the model predicts 100% background\n",
    "    should it be be 80% right (as with categorical cross entropy)\n",
    "    or 30% (with this loss)?\n",
    "    The loss has been modified to have a smooth gradient as it converges on zero.\n",
    "    This has been shifted so it converges on 0 and is smoothed to avoid exploding\n",
    "    or disappearing gradient.\n",
    "    Jaccard = (|X & Y|)/ (|X|+ |Y| - |X & Y|)\n",
    "            = sum(|A*B|)/(sum(|A|)+sum(|B|)-sum(|A*B|))\n",
    "    # Arguments\n",
    "        y_true: The ground truth tensor.\n",
    "        y_pred: The predicted tensor\n",
    "        smooth: Smoothing factor. Default is 100.\n",
    "    # Returns\n",
    "        The Jaccard distance between the two tensors.\n",
    "    # References\n",
    "        - [What is a good evaluation measure for semantic segmentation?](\n",
    "           http://www.bmva.org/bmvc/2013/Papers/paper0032/paper0032.pdf)\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only need 1 dimension for labels. reserve 25 images as a test set\n",
    "labelstemp=resized_labels[:-100,...,1].reshape([labels.shape[0]-100,224,224,1])\n",
    "datasettemp=resized_dataset[:-100,...].reshape([labels.shape[0]-100,224,224,3])\n",
    "datasetnice=resized_dataset[:-100,...].reshape([labels.shape[0]-100,224,224,3])\n",
    "#labelstemp=np.sum(datasettemp, axis=3) <= np.mean(np.sum(datasettemp, axis=3), axis=(1,2))[:,None,None]/2\n",
    "#labelstemp=labelstemp.reshape([labels.shape[0]-25,224,224,1])\n",
    "#datasettemp=np.sum(datasettemp, axis=3) >= np.mean(np.sum(datasettemp, axis=3), axis=(1,2))[:,None,None]/2\n",
    "#datasettemp=datasettemp.reshape([labels.shape[0]-25,648,648,1])\n",
    "\n",
    "#add all 4 rotations of 90 degrees to augment data\n",
    "labelstrain=np.copy(labelstemp) #arrays used to train model\n",
    "datasettrain=np.copy(datasettemp)\n",
    "for i in range(3):\n",
    "    labelstrain=np.append(labelstrain, np.rot90(labelstemp, k=i+1, axes=(1,2)), axis=0)\n",
    "    datasettrain=np.append(datasettrain, np.rot90(datasettemp, k=i+1, axes=(1,2)), axis=0)\n",
    "\n",
    "print(datasettrain.shape)\n",
    "print(labelstrain.shape)\n",
    "f = plt.figure()\n",
    "image=573\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(datasettrain[image,:,:,:].astype('float32')/2+[.5])\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(labelstrain[image,:,:,0])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#training!\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3,4,5,6,7,8\"\n",
    "no_gpus=8\n",
    "smallest_layer=32\n",
    "if no_gpus > 1:\n",
    "    modelrough=multi_gpu_model(resnet152_model(1, weights_path='/home/ubuntu/MPhys-Project/model_weights/rough_model_weights.h5'), gpus=no_gpus)\n",
    "#model=RESUNET(dataset.shape[1:],2)\n",
    "else:\n",
    "    modelrough=resnet152_model(1, weights_path='/home/ubuntu/MPhys-Project/model_weights/rough_model_weights.h5')\n",
    "#model=multi_gpu_model(model,gpus=8)\n",
    "\n",
    "from keras.metrics import categorical_accuracy\n",
    "optimizer=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "#optimizer=rmsprop(lr=0.0001)\n",
    "#loss=create_weighted_binary_crossentropy(0.01,0.99)\n",
    "#loss=jaccard_distance\n",
    "loss=dice_coef_loss\n",
    "#loss='binary_crossentropy'\n",
    "modelrough.compile(optimizer=optimizer,\n",
    "                         loss=loss, metrics=[dice_coef, 'accuracy', 'binary_crossentropy'])\n",
    "print(modelrough.summary())\n",
    "#model.load_weights('/home/ubuntu/MPhys-Project/model_weights/rough_model_weights.h5')\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the refined mask model and load the weights\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1,2,3,4,5,6,7,8\"\n",
    "no_gpus=1\n",
    "smallest_layer=32\n",
    "if no_gpus > 1:\n",
    "    refined_mask_model=multi_gpu_model(RESUNETMEDIUM((224,224,3), 1), gpus=no_gpus)\n",
    "else:\n",
    "    refined_mask_model=RESUNETMEDIUM((224,224,3), 1)\n",
    "\n",
    "from keras.metrics import categorical_accuracy\n",
    "optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "loss=dice_coef_loss\n",
    "refined_mask_model.compile(optimizer=optimizer,\n",
    "                         loss=loss, metrics=[dice_coef, 'accuracy', 'binary_crossentropy'])\n",
    "\n",
    "refined_mask_model.load_weights('/home/ubuntu/MPhys-Project/model_weights/refine_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelrough.load_weights('all_data_best_weights.h5')\n",
    "mask=modelrough.predict(resized_dataset, batch_size=8*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the masks and dilate them, then apply them to the dataset\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 6))\n",
    "dilated_mask=np.zeros(mask.shape)\n",
    "open_closed_masks = np.zeros((mask.shape), dtype='float32')\n",
    "for i in range(mask.shape[0]):\n",
    "    open_closed_masks[i,...,0] = cv2.morphologyEx(cv2.morphologyEx(mask[i,...,0].astype('float32'), cv2.MORPH_OPEN, kernel), cv2.MORPH_CLOSE, kernel)\n",
    "    dilated_mask[i,...,0]=cv2.dilate(open_closed_masks[i,...,0], kernel, iterations=6)>=0.5\n",
    "f = plt.figure(figsize=(18,18))\n",
    "datasetmasked = np.zeros(resized_dataset.shape)\n",
    "labelsmasked = resized_labels*dilated_mask\n",
    "for i in range(3):\n",
    "    datasetmasked[...,i]=(resized_dataset[...,i]+1)/2*dilated_mask[...,0]\n",
    "\n",
    "image=45\n",
    "f.add_subplot(1,6, 1)\n",
    "plt.imshow(mask[image,:,:,0])\n",
    "f.add_subplot(1,6, 2)\n",
    "plt.imshow(dilated_mask[image,:,:,0])\n",
    "f.add_subplot(1,6, 3)\n",
    "plt.imshow(resized_dataset[image,:,:,:].astype('float32')/2+[.5])\n",
    "f.add_subplot(1,6, 4)\n",
    "plt.imshow(resized_labels[image,:,:,1])\n",
    "f.add_subplot(1,6, 5)\n",
    "plt.imshow(datasetmasked[image,:,:,:].astype('float32'))\n",
    "f.add_subplot(1,6, 6)\n",
    "plt.imshow(labelsmasked[image,:,:,1])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model.fit(x_train, y_train, batch_size=100, epochs=3)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1,) #min_lr=5e-7, verbose=1)\n",
    "#earlystopper = EarlyStopping(patience=8, verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath='Checkpoint_Weights.h5', save_best_only=True, monitor='val_acc', verbose=1)\n",
    "\n",
    "batch_size=1*no_gpus\n",
    "history = model.fit(dataset_threshold, tru, batch_size=batch_size, epochs=6, callbacks=[reduce_lr, checkpointer], validation_split=0.2)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manual shuffle just in case\n",
    "def shuffle_in_unison(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
    "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
    "    permutation = np.random.permutation(len(a))\n",
    "    for old_index, new_index in enumerate(permutation):\n",
    "        shuffled_a[new_index] = a[old_index]\n",
    "        shuffled_b[new_index] = b[old_index]\n",
    "    return shuffled_a, shuffled_b\n",
    "\n",
    "datasettrain, labelstrain=shuffle_in_unison(datasettrain,labelstrain)\n",
    "print(datasettrain.shape[0])\n",
    "print(labelstrain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights('Best_Weights.h5')\n",
    "#model.load_weights('resnet_152_smaller_best_weights.h5')\n",
    "modelrefine.load_weights('modelrefine_weights_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "image=109\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(datasettrain[image,:,:,:].astype('float32')/2+[.5])\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(labelstrain[image,:,:,0])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training with data augmentation actually implemented properly. This was not used in the end.\n",
    "\n",
    "model.load_weights('all_data_best_weights_rotations.h5')\n",
    "batch_size_per_gpu=6\n",
    "batch_size=batch_size_per_gpu*no_gpus\n",
    "validation_split=0.2\n",
    "\n",
    "data=datasettrain\n",
    "truth=labelstrain\n",
    "\n",
    "epochs=50\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    #rotation_range=15,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2,\n",
    "    #shear_range=0.2,\n",
    "    #horizontal_flip=True,\n",
    "    validation_split=validation_split)\n",
    "\n",
    "\n",
    "\n",
    "# we create two instances with the same arguments\n",
    "data_gen_args = dict(\n",
    "                    rotation_range=45,\n",
    "                    #width_shift_range=0.2,\n",
    "                    #height_shift_range=0.2,\n",
    "                    #shear_range=0.2,\n",
    "                    horizontal_flip=True,\n",
    "                    zoom_range=0.2,)\n",
    "                    #validation_split=validation_split)\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "# Provide the same seed and keyword arguments to the fit and flow methods\n",
    "seed = 1\n",
    "image_datagen.fit(data, augment=True, seed=seed)\n",
    "mask_datagen.fit(truth, augment=True, seed=seed)\n",
    "\n",
    "image_generator_train = image_datagen.flow(data, batch_size=batch_size, shuffle=True, seed=seed,)# subset='training')\n",
    "mask_generator_train = image_datagen.flow(truth, batch_size=batch_size, shuffle=True, seed=seed,)# subset='training')\n",
    "#image_generator_validate = image_datagen.flow(data, batch_size=batch_size, shuffle=True, seed=seed, subset='validation')\n",
    "#mask_generator_validate = image_datagen.flow(truth, batch_size=batch_size, shuffle=True, seed=seed, subset='validation')\n",
    "\n",
    "# combine generators into one which yields image and masks\n",
    "train_generator = zip(image_generator_train, mask_generator_train)\n",
    "#validation_generator = zip(image_generator_validate, mask_generator_validate)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=6, min_lr=5e-9, verbose=1)\n",
    "#earlystopper = EarlyStopping(patience=20, verbose=1, monitor='val_loss')\n",
    "checkpointer = ModelCheckpoint(filepath='model_weights_full_data.h5', save_best_only=True, monitor='loss', verbose=1)\n",
    "\n",
    "if no_gpus == 1:\n",
    "    history = model.fit_generator(train_generator,\n",
    "                    epochs=epochs, callbacks=[reduce_lr, checkpointer], steps_per_epoch=data.shape[0]//batch_size,)\n",
    "                    #validation_data=validation_generator,\n",
    "                    #validation_steps=data.shape[0]*validation_split//batch_size,)#, use_multiprocessing=True)\n",
    "else:\n",
    "    history = model.fit_generator(train_generator,\n",
    "                    epochs=epochs, callbacks=[reduce_lr, checkpointer], steps_per_epoch=data.shape[0]//batch_size,)\n",
    "                    #validation_data=validation_generator,\n",
    "                    #validation_steps=data.shape[0]*validation_split//batch_size_per_gpu, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(len(history.history['loss']))\n",
    "plt.plot(epochs, history.history['loss'], label='loss')\n",
    "#plt.plot(epochs, history.history['val_loss'], label='val_loss')\n",
    "#plt.plot(epochs, history.history['dice_coef'], label='dice_coef')\n",
    "#plt.plot(epochs, history.history['val_dice_coef'], label='val_dice_coef')\n",
    "plt.plot(epochs, history.history['acc'], label='accuracy')\n",
    "#plt.plot(epochs, history.history['val_acc'], label='val_accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"resnet152_smaller.h5\")\n",
    "model.save_weights(\"rough_model_all_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(history, 'training_history.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model.load_weights('all_data_best_weights_rotations_longer_training.h5')\n",
    "#Plot some examples\n",
    "no_gpus = 1\n",
    "f = plt.figure(figsize=(18,300))\n",
    "samples=datasetmasked[resized_labels[...,0].any(axis=(1,2)),...][0:100:5,:,:,:].astype('float32').reshape(20,224,224,3)\n",
    "original_images=resized_dataset[resized_labels[...,0].any(axis=(1,2)),...][0:100:5,:,:,:].astype('float32').reshape(20,224,224,3)\n",
    "annotations=resized_labels[resized_labels[...,0].any(axis=(1,2)),...,0][0:100:5,:,:]\n",
    "prediction=refined_mask_model.predict(samples, batch_size=no_gpus*8)\n",
    "for image, sample in enumerate(samples):\n",
    "    f.add_subplot(samples.shape[0],5, 5*image+1)\n",
    "    plt.imshow(np.clip(original_images[image,:,:,:]/2+0.5, 0, 1))\n",
    "    f.add_subplot(samples.shape[0],5, 5*image+2)\n",
    "    plt.imshow(np.clip(samples[image,:,:,:], 0, 1))\n",
    "    f.add_subplot(samples.shape[0],5, 5*image+3)\n",
    "    plt.imshow(annotations[image,:,:])\n",
    "    f.add_subplot(samples.shape[0],5, 5*image+4)\n",
    "    plt.imshow(prediction[image,:,:,0]>=0.5)\n",
    "    f.add_subplot(samples.shape[0],5, 5*image+5)\n",
    "    plt.imshow(open_closed_masks[image,:,:,0]>=0.5)\n",
    "f.tight_layout()\n",
    "f.subplots_adjust(hspace=0.1)\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=2*dataset.astype('float16')-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model metrics\n",
    "\n",
    "#model.load_weights('all_data_best_weights.h5')\n",
    "pre = refined_mask_model.predict(datasetmasked[-100:,...], batch_size=8*no_gpus)>=0.5\n",
    "print(pre.shape)\n",
    "tru = resized_labels[-100:,...,1].reshape(100,224,224,1).astype('bool')\n",
    "\n",
    "true_positives = tru*pre\n",
    "false_positives = pre*~tru\n",
    "true_negatives = ~tru*~pre\n",
    "false_negatives = tru*~pre\n",
    "\n",
    "tot_tp = np.sum(true_positives)\n",
    "tot_fp = np.sum(false_positives)\n",
    "tot_tn = np.sum(true_negatives)\n",
    "tot_fn = np.sum(false_negatives)\n",
    "\n",
    "precision = tot_tp/(tot_tp+tot_fp)\n",
    "recall = tot_tp/(tot_tp+tot_fn)\n",
    "f1 = 2*precision*recall/(precision + recall)\n",
    "TPR = recall #recall is the same as true positive rate\n",
    "TNR = tot_tn/(tot_tn+tot_fp) #true negative rate\n",
    "bACC = (TPR+TNR)/2 #Balanced accuracy score\n",
    "\n",
    "print('precision = ', precision)\n",
    "print('recall = ', recall)\n",
    "print('f1 score = ', f1)\n",
    "print('true positive rate = ', TPR)\n",
    "print('true negative rate = ', TNR)\n",
    "print('balanced accuracy score = ', bACC)\n",
    "\n",
    "pre = pre[...,0].any(axis=(1,2))\n",
    "tru = tru[..., 0].any(axis=(1,2))\n",
    "\n",
    "true_positives = tru*pre\n",
    "false_positives = pre*~tru\n",
    "true_negatives = ~tru*~pre\n",
    "false_negatives = tru*~pre\n",
    "\n",
    "tot_tp = np.sum(true_positives)\n",
    "tot_fp = np.sum(false_positives)\n",
    "tot_tn = np.sum(true_negatives)\n",
    "tot_fn = np.sum(false_negatives)\n",
    "\n",
    "precision = tot_tp/(tot_tp+tot_fp)\n",
    "recall = tot_tp/(tot_tp+tot_fn)\n",
    "f1 = 2*precision*recall/(precision + recall)\n",
    "TPR = recall #recall is the same as true positive rate\n",
    "TNR = tot_tn/(tot_tn+tot_fp) #true negative rate\n",
    "bACC = (TPR+TNR)/2 #Balanced accuracy score\n",
    "\n",
    "print('image precision = ', precision)\n",
    "print('image recall = ', recall)\n",
    "print('image f1 score = ', f1)\n",
    "print('image true positive rate = ', TPR)\n",
    "print('image true negative rate = ', TNR)\n",
    "print('image balanced accuracy score = ', bACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = labelsnew.tolist()\n",
    "y_pred = model.predict(dataset, batch_size=batch_size)\n",
    "target_names = ['not POC', 'POC']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.eye(3).reshape((3,3,1))\n",
    "b=np.copy(a)\n",
    "for i in range(3):\n",
    "    b=np.append(b, np.rot90(a, k=i+1), axis=2)\n",
    "    print(i)\n",
    "print(b[:,:,])\n",
    "print(a[:,:,0])\n",
    "print(b.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = np.array([True, True, False, False])\n",
    "tru = np.array([False, True, True, False])\n",
    "\n",
    "print('tp = ', tru*pre)\n",
    "print('fp = ', pre*~tru)\n",
    "print('tn = ', ~tru * ~pre)\n",
    "print('fn = ', tru * ~ pre)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "a.resize(2,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amax(resized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_dataset.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetmasked.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}